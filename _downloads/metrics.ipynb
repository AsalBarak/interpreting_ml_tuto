{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nMetrics to judge the sucess of a model\n=======================================\n\nPro & cons of various performance metrics.\n\nThe simple way to use a scoring metric during cross-validation is\nvia the `scoring` parameter of\n:func:`sklearn.model_selection.cross_val_score`.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Regression settings\n-----------------------\n\nThe Boston housing data\n........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import datasets\nboston = datasets.load_boston()\n\n# Shuffle the data\nfrom sklearn.utils import shuffle\ndata, target = shuffle(boston.data, boston.target, random_state=0)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "A quick plot of how each feature is related to the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\n\nfor feature, name in zip(data.T, boston.feature_names):\n    plt.figure(figsize=(4, 3))\n    plt.scatter(feature, target)\n    plt.xlabel(name, size=22)\n    plt.ylabel('Price (US$)', size=22)\n    plt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We will be using a random forest regressor to predict the price\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nmodel = RandomForestRegressor()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance vs Mean Square Error\n.......................................\n\nThe default score is explained variance\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(model, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Explained variance is convienent because it has a natural scaling: 1 is\nperfect prediction, and 0 is around chance\n\nNow let us see which houses are easier to predict:\n\nNot along the Charles river (feature 3)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(model, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(model, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1]))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the houses along the Charles are harder to predict?\n\nIt's not so easy to conclude this from the explained variance: in two\ndifferent sets of observations, the variance of the target differs, and\nthe explained variance is a relative measure\n\n**MSE**: We can use the mean squared error (here negated)\n\nNot along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(model, data[data[:, 3] == 0],\n                      target[data[:, 3] == 0],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(model, data[data[:, 3] == 1],\n                      target[data[:, 3] == 1],\n                      scoring='neg_mean_squared_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "So the error is larger along the Charles river\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Mean Squared Error versus Mean Absolute Error\n..................................................\n\nWhat if we want to report an error in dollars, meaningful for an\napplication?\n\nThe Mean Absolute Error is useful for this goal\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "print(cross_val_score(model, data, target,\n                      scoring='neg_mean_absolute_error'))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Summary\n.........\n\n* **explained variance**: scaled with regards to chance: 1 = perfect,\n  0 = around chance, but it shouldn't used to compare predictions\n  across datasets\n\n* **mean absolute error**: enables comparison across datasets in the\n  units of the target\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Classification settings\n-----------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# Accuracy and its shortcomings\n\n# precision, recall, and their shortcomings\n\n# ROC AUC\n\n# average precision"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.14", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}