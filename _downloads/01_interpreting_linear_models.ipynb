{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nInterpreting linear models\n==========================\n\nLinear models are not that easy to interpret when variables are\ncorrelated.\n\nSee also the `statistics chapter\n<http://www.scipy-lectures.org/packages/statistics/index.html>`_ of the\n`scipy lecture notes <http://www.scipy-lectures.org>`_\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Data on wages\n--------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import os\nimport pandas\n\n# Python 2 vs Python 3:\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\n\nif not os.path.exists('wages.txt'):\n    # Download the file if it is not present\n    urlretrieve('http://lib.stat.cmu.edu/datasets/CPS_85_Wages',\n                'wages.txt')\n\n# Give names to the columns\nnames = [\n    'EDUCATION: Number of years of education',\n    'SOUTH: 1=Person lives in South, 0=Person lives elsewhere',\n    'SEX: 1=Female, 0=Male',\n    'EXPERIENCE: Number of years of work experience',\n    'UNION: 1=Union member, 0=Not union member',\n    'WAGE: Wage (dollars per hour)',\n    'AGE: years',\n    'RACE: 1=Other, 2=Hispanic, 3=White',\n    'OCCUPATION: 1=Management, 2=Sales, 3=Clerical, 4=Service, 5=Professional, 6=Other',\n    'SECTOR: 0=Other, 1=Manufacturing, 2=Construction',\n    'MARR: 0=Unmarried,  1=Married',\n]\n\nshort_names = [n.split(':')[0] for n in names]\n\ndata = pandas.read_csv('wages.txt', skiprows=27, skipfooter=6, sep=None,\n                       header=None)\ndata.columns = short_names\n\n# Log-transform the wages, as they typically increase with\n# multiplicative factors\nimport numpy as np\ndata['WAGE'] = np.log10(data['WAGE'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The challenge of correlated features\n--------------------------------------------\n\nPlot scatter matrices highlighting different aspects\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import seaborn\nseaborn.pairplot(data, vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note that age and experience are highly correlated\n\nA link between a single feature and the target is a *marginal* link.\n\n\nUnivariate feature selection selects on marginal links.\n\nLinear model compute *conditional* links: removing the effects of other\nfeatures on each feature. This is hard when features are correlated.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Coefficients of a linear model\n--------------------------------------------\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import linear_model\nfeatures = [c for c in data.columns if c != 'WAGE']\nX = data[features]\ny = data['WAGE']\nridge = linear_model.RidgeCV()\nridge.fit(X, y)\n\n# Visualize the coefs\ncoefs = ridge.coef_\nfrom matplotlib import pyplot as plt\nplt.barh(np.arange(coefs.size), coefs)\nplt.yticks(np.arange(coefs.size), features)\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note: coefs cannot easily be compared if X is not standardized: they\nshould be normalized to the variance of X.\n\nWhen features are not too correlated and their is plenty, this is the\nwell-known regime of standard statistics in linear models. Machine\nlearning is not needs, and statsmodels is a great tool (see the\n`statistics chapter in scipy-lectures\n<http://www.scipy-lectures.org/packages/statistics/index.html>`_)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The effect of regularization\n--------------------------------------------\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "lasso = linear_model.LassoCV()\nlasso.fit(X, y)\n\ncoefs = lasso.coef_\nfrom matplotlib import pyplot as plt\nplt.barh(np.arange(coefs.size), coefs)\nplt.yticks(np.arange(coefs.size), features)\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "l1 regularization (sparse models) puts variables to zero.\n\nWhen two variables are very correlated, it will put arbitrary one or\nthe other to zero depending on their SNR. Here we can see that age\nprobably overshadowed experience.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Stability to gauge significance\n--------------------------------\n\nStability of coefficients when perturbing the data helps giving an\ninformal evaluation of the significance of the coefficients. Note that\nthis is not significance testing in the sense of p-values, as a model\nthat returns coefficients always at one indepently of the data will\nappear as very stable though it clearly does not control for false\ndetections.\n\nWe can do this in a cross-validation loop, using the argument\n\"return_estimator\" of :func:`sklearn.model_selection.cross_validate`\nwhich has been added in version 0.20 of scikit-learn:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_validate"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "With the lasso estimator\n.........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "cv_lasso = cross_validate(lasso, X, y, return_estimator=True, cv=7)\ncoefs_ = [estimator.coef_ for estimator in cv_lasso['estimator']]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Plot the results with seaborn:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "coefs_ = pandas.DataFrame(coefs_, columns=features)\nseaborn.boxplot(data=coefs_, orient='h')\nseaborn.stripplot(data=coefs_, orient='h')\nplt.axvline(x=0, color='.5')  # Add a vertical line at 0\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "With the ridge estimator\n........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "cv_ridge = cross_validate(ridge, X, y, return_estimator=True, cv=7)\ncoefs_ = [estimator.coef_ for estimator in cv_ridge['estimator']]\n\ncoefs_ = pandas.DataFrame(coefs_, columns=features)\nseaborn.boxplot(data=coefs_, orient='h')\nseaborn.stripplot(data=coefs_, orient='h')\nplt.axvline(x=0, color='.5') # Add a vertical line at 0\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Which is the truth?\n....................\n\nNote the difference between the lasso and the ridge estimator: we do\nnot have enough data to perfectly estimate conditional relationships,\nhence the prior (ie the regularization) makes a difference, and its is\nhard to tell from the data which is the \"truth\".\n\nOne reasonnable model-selection criterion is to believe most the model\nthat predicts best. For this, we can inspect the prediction scores\nobtained via the cross-validation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "scores = pandas.DataFrame({'lasso': cv_lasso['test_score'],\n                           'ridge': cv_ridge['test_score']})\nseaborn.boxplot(data=scores, orient='h')\nseaborn.stripplot(data=scores, orient='h')"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note also that the limitations of cross-validation explained previously\nstill apply. Ideally, we should use a ShuffleSplit cross-validation\nobject to sample many times and have a better estimate of the\nposterior, both for the coefficients and the test scores.\n\n_____________________\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}