{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nInterpreting linear models\n==========================\n\nLinear models are not that easy to interpret when variables are\ncorrelated.\n\nSee also the `statistics chapter\n<http://www.scipy-lectures.org/packages/statistics/index.html>`_ of the\n`scipy lecture notes <http://www.scipy-lectures.org>`_\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Data on wages\n--------------\n\nWe first download and load some historical data on wages\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import os\nimport pandas\n\n# Python 2 vs Python 3:\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\nif not os.path.exists('wages.txt'):\n    # Download the file if it is not present\n    urlretrieve('http://lib.stat.cmu.edu/datasets/CPS_85_Wages',\n                'wages.txt')\n\n# Give names to the columns\nnames = [\n    'EDUCATION: Number of years of education',\n    'SOUTH: 1=Person lives in South, 0=Person lives elsewhere',\n    'SEX: 1=Female, 0=Male',\n    'EXPERIENCE: Number of years of work experience',\n    'UNION: 1=Union member, 0=Not union member',\n    'WAGE: Wage (dollars per hour)',\n    'AGE: years',\n    'RACE: 1=Other, 2=Hispanic, 3=White',\n    'OCCUPATION: 1=Management, 2=Sales, 3=Clerical, 4=Service, 5=Professional, 6=Other',\n    'SECTOR: 0=Other, 1=Manufacturing, 2=Construction',\n    'MARR: 0=Unmarried,  1=Married',\n]\n\nshort_names = [n.split(':')[0] for n in names]\ndata = pandas.read_csv('wages.txt', skiprows=27, skipfooter=6, sep=None,\n                       header=None)\ndata.columns = short_names\n\n# Log-transform the wages, as they typically increase with\n# multiplicative factors\nimport numpy as np\ndata['WAGE'] = np.log10(data['WAGE'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The challenge of correlated features\n--------------------------------------------\n\nPlot scatter matrices highlighting the links between different\nvariables measured\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import seaborn\n# The simplest way to plot a pairplot\nseaborn.pairplot(data, vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE', 'SEX'])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "A fancier pair plot\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\n\ng = seaborn.PairGrid(data,\n                     vars=['WAGE', 'AGE', 'EDUCATION', 'EXPERIENCE', 'SEX'],\n                     diag_sharey=False)\ng.map_upper(seaborn.kdeplot)\ng.map_lower(plt.scatter, s=2)\ng.map_diag(seaborn.kdeplot, lw=3)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note that age and experience are highly correlated\n\nA link between a single feature and the target is a *marginal* link.\n\n\nUnivariate feature selection selects on marginal links.\n\nLinear model compute *conditional* links: removing the effects of other\nfeatures on each feature. This is hard when features are correlated.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Coefficients of a linear model\n--------------------------------------------\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import linear_model\nfeatures = [c for c in data.columns if c != 'WAGE']\nX = data[features]\ny = data['WAGE']\nridge = linear_model.RidgeCV()\nridge.fit(X, y)\n\n# Visualize the coefs\ncoefs = ridge.coef_\nfrom matplotlib import pyplot as plt\nplt.figure(figsize=(6, 4))\nplt.barh(np.arange(coefs.size), coefs)\nplt.yticks(np.arange(coefs.size), features)\nplt.title(\"Coefficients\")\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Scaling coefficients**: coefs cannot easily be compared if X is not\nstandardized: they should be normalized to the variance of X: the\ngreater the variance of a feature, the large the impact of the\ncorresponding coefficent on the output.\n\nIf the different features have differing, possibly arbitrary, scales,\nthen scaling the coefficients by the feature scale often helps\ninterpretation.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "X_std = X.std()\nplt.figure(figsize=(6, 4))\nplt.barh(np.arange(coefs.size), coefs * X_std)\nplt.yticks(np.arange(coefs.size), features)\nplt.title(\"Scaled coefficients\")\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Now the age and experience can be better compared: and experience does\nappear as more important than age.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "When features are not too correlated and there is plenty of samples,\nthis is the well-known regime of standard statistics in linear models.\nMachine learning is not needed, and statsmodels is a great tool (see the\n`statistics chapter in scipy-lectures\n<http://www.scipy-lectures.org/packages/statistics/index.html>`_)\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The effect of regularization\n--------------------------------------------\n\nSparse models use l1 regularization to puts some variables to\nzero. This can often help interpretation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "lasso = linear_model.LassoCV()\nlasso.fit(X, y)\n\ncoefs = lasso.coef_\nplt.barh(np.arange(coefs.size), coefs * X_std)\nplt.yticks(np.arange(coefs.size), features)\nplt.title(\"Sparse coefficients\")\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "When two variables are very correlated (such as age and experience), it\nwill put arbitrarily one or the other to zero depending on their SNR.\n\nHere we can see that age probably overshadowed experience.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Stability to gauge significance\n--------------------------------\n\nStability of coefficients when perturbing the data helps giving an\ninformal evaluation of the significance of the coefficients. Note that\nthis is not significance testing in the sense of p-values, as a model\nthat returns coefficients always at one indepently of the data will\nappear as very stable though it clearly does not control for false\ndetections.\n\nWe can do this in a cross-validation loop, using the argument\n\"return_estimator\" of :func:`sklearn.model_selection.cross_validate`\nwhich has been added in version 0.20 of scikit-learn:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_validate"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "With the lasso estimator\n.........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "cv_lasso = cross_validate(lasso, X, y, return_estimator=True, cv=10)\ncoefs_ = [estimator.coef_ for estimator in cv_lasso['estimator']]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Plot the results with seaborn:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "coefs_ = pandas.DataFrame(coefs_, columns=features) * X_std\nplt.figure(figsize=(6, 4))\nseaborn.boxplot(data=coefs_, orient='h')\nseaborn.stripplot(data=coefs_, orient='h', color='k')\nplt.axvline(x=0, color='.5')  # Add a vertical line at 0\nplt.title('Sparse coefficients')\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "With the ridge estimator\n........................\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "cv_ridge = cross_validate(ridge, X, y, return_estimator=True, cv=10)\ncoefs_ = [estimator.coef_ for estimator in cv_ridge['estimator']]\n\ncoefs_ = pandas.DataFrame(coefs_, columns=features) * X_std\nplt.figure(figsize=(6, 4))\nseaborn.boxplot(data=coefs_, orient='h')\nseaborn.stripplot(data=coefs_, orient='h', color='k')\nplt.axvline(x=0, color='.5') # Add a vertical line at 0\nplt.title('Non-sparse coefficients (ridge)')\nplt.tight_layout()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The story is different. Note also that lasso coefficients are much more\ninstable than the ridge. It is often the case that sparse models are\nunstable.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Which is the truth?\n....................\n\nNote the difference between the lasso and the ridge estimator: we do\nnot have enough data to perfectly estimate conditional relationships,\nhence the prior (ie the regularization) makes a difference, and its is\nhard to tell from the data which is the \"truth\".\n\nOne reasonnable model-selection criterion is to believe most the model\nthat predicts best. For this, we can inspect the prediction scores\nobtained via the cross-validation\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "scores = pandas.DataFrame({'lasso': cv_lasso['test_score'],\n                           'ridge': cv_ridge['test_score']})\nplt.figure(figsize=(6, 2))\nseaborn.boxplot(data=scores, orient='h')\nseaborn.stripplot(data=scores, orient='h', color='k')\nplt.title(\"Model comparison\")"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Note also that the limitations of cross-validation explained previously\nstill apply. Ideally, we should use a ShuffleSplit cross-validation\nobject to sample many times and have a better estimate of the\nposterior, both for the coefficients and the test scores.\n\nConclusion on factors of wages?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs always, concluding is hard. That said, it seems that we should\nprefer the scaled ridge coefficients.\n\nHence, the most important factors of wage are eduction and\nexperience, followed by sex: at the same education and experience\nfemales earn less than males. Note that this last statement is a\nstatement about the link between wage and sex, conditional on education\nand experience.\n\n_____________________\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}