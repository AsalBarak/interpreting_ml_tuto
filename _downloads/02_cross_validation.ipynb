{
  "nbformat_minor": 0, 
  "nbformat": 4, 
  "cells": [
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "%matplotlib inline"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "\nCross-validation: some gotchas\n===============================\n\nCross-validation is the ubiquitous test of a machine learning model. Yet\nmany things can go wrong.\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "The uncertainty of measured accuracy\n------------------------------------\n\nThe first thing to have in mind is that the results of a\ncross-validation are noisy estimate of the real prediction accuracy\n\nLet us create a simple artificial data\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import datasets, discriminant_analysis\nimport numpy as np\nnp.random.seed(0)\ndata, target = datasets.make_blobs(centers=[(0, 0), (0, 1)])\nclassifier = discriminant_analysis.LinearDiscriminantAnalysis()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "One cross-validation gives spread out measures\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import cross_val_score\nprint(cross_val_score(classifier, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "What if we try different random shuffles of the data?\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import utils\nfor _ in range(10):\n    data, target = utils.shuffle(data, target)\n    print(cross_val_score(classifier, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "This should not be surprising: if the classification rate is p, the\nobserved distribution of correct classifications on a set of size\nfollows a binomial distribution\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from scipy import stats\nn = len(data)\ndistrib = stats.binom(n=n, p=.7)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We can plot it:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from matplotlib import pyplot as plt\nplt.figure(figsize=(6, 3))\nplt.plot(np.linspace(0, 1, n), distrib.pmf(np.arange(0, n)))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "It is wide, because there are not that many samples to mesure the error\nupon: this is a small dataset.\n\nWe can look at the interval in which 95% of the observed accuracy lies\nfor different sample sizes\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "for n in [100, 1000, 10000, 100000, 1000000]:\n    distrib = stats.binom(n, .7)\n    interval = (distrib.isf(.025) - distrib.isf(.975)) / n\n    print(\"Size: {0: 8}  | interval: {1}%\".format(n, 100 * interval))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "At 100 000 samples, 5% of the observed classification accuracy still\nfall more than .5% away of the true rate.\n\n**Keep in mind that cross-val is a noisy measure**\n\nImportantly, the variance across folds is not a good measure of this\nerror, as the different data folds are not independent. For instance,\ndoing many random splits will can reduce the variance arbitrarily, but\ndoes not provide actually new data points\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "Measuring baselines and chance\n-------------------------------\n\nBecause of class imbalances, or confounding effects, it is easy to get\nit wrong it terms of what constitutes chances. There are two approaches\nto measure peformances of baselines or chance:\n\nLet's go back to simple generated data:\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "data, target = datasets.make_blobs(centers=[(0, 0), (0, 1)])"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**DummyClassifier** The dummy classifier:\n:class:`sklearn.dummy.DummyClassifier`, with different strategies to\nprovide simple baselines\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.dummy import DummyClassifier\ndummy = DummyClassifier(strategy=\"stratified\")\nprint(cross_val_score(dummy, data, target))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "**Chance level** To measure actual chance, the most robust approach is\nto use permutations:\n:func:`sklearn.model_selection.permutation_test_score`, which is used\nas cross_val_score\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn.model_selection import permutation_test_score\nscore, permuted_scores, p_value = permutation_test_score(classifier, data, target)\nprint(\"Classifier score: {0},\\np value: {1}\\nPermutation scores {2}\"\n        .format(score, p_value, permuted_scores))"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Cross-validation with non iid data\n-----------------------------------\n\nStock market: time series\n...........................\n\nDownload and load the data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import pandas as pd\nimport os\n# Python 2 vs Python 3:\ntry:\n    from urllib.request import urlretrieve\nexcept ImportError:\n    from urllib import urlretrieve\n\nsymbols = {'TOT': 'Total', 'XOM': 'Exxon', 'CVX': 'Chevron',\n           'COP': 'ConocoPhillips', 'VLO': 'Valero Energy'}\n\nquotes = pd.DataFrame()\n\nfor symbol, name in symbols.items():\n    url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'\n           'master/financial-data/{}.csv')\n    filename = \"{}.csv\".format(symbol)\n    if not os.path.exists(filename):\n        urlretrieve(url.format(symbol), filename)\n    this_quote = pd.read_csv(filename)\n    quotes[name] = this_quote['open']"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Predict 'Chevron' from the others\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import linear_model, model_selection, ensemble\ncv = model_selection.ShuffleSplit(random_state=0)\nprint(cross_val_score(linear_model.RidgeCV(),\n                      quotes.drop(columns=['Chevron']),\n                      quotes['Chevron'],\n                      cv=cv).mean())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Is this a robust prediction?\n\nDoes it cary over across quarters?\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "quarters = pd.to_datetime(this_quote['date']).dt.to_period('Q')\ncv = model_selection.LeaveOneGroupOut()\n\nprint(cross_val_score(linear_model.RidgeCV(),\n                      quotes.drop(columns=['Chevron']),\n                      quotes['Chevron'],\n                      cv=cv, groups=quarters).mean())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "The problem that we are facing here is the auto-correlation in the\ndata: these datasets are **time-series**.\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "quotes_with_dates = pd.concat((quotes, this_quote['date']),\n                              axis=1).set_index('date')\nquotes_with_dates.plot()"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "If the goal is to do forecasting, than prediction should be done in the\nfuture, for instance using\n:class:`sklearn.model_selection.TimeSeriesSplit`\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "School grades: repeated measures\n.................................\n\nLet us look at another dependency structure across samples: repeated\nmeasures. This is often often in longitudinal data. Here we are looking\nat grades of school students, across the years.\n\nDownload and load the data\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nDownload some data on grades across several schools (centers)\n\nThe junior school data, originally from http://www.bristol.ac.uk/cmm/learning/support/datasets/\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "if not os.path.exists('exams.csv.gz'):\n    # Download the file if it is not present\n    urlretrieve('https://raw.githubusercontent.com/GaelVaroquaux/interpreting_ml_tuto/blob/master/src/01_how_well/exams.csv.gz',\n                filename)\nexams = pd.read_csv('exams.csv.gz')\n\n# Select data for students present all three years\ncontinuing_students = exams.StudentID.value_counts()\ncontinuing_students = continuing_students[continuing_students > 2].index\nexams = exams[exams.StudentID.isin(continuing_students)]"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Visualized factor of grades\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGrade at tests in in exams depend on socio-economic status, year at\nschool, ...\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "import seaborn as sns\ng = sns.PairGrid(exams.drop(columns=['StudentID']),\n                 diag_sharey=False)\ng.map_lower(sns.kdeplot)\ng.map_upper(plt.scatter, s=2)\ng.map_diag(sns.kdeplot, lw=3)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "A zoomed view on the factors that seem most interpretable\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "g = sns.PairGrid(exams[['Ravens', 'Maths', 'English', 'SocialClass', 'Year']],\n                 diag_sharey=False)\ng.map_lower(sns.kdeplot)\ng.map_upper(plt.scatter, s=2)\ng.map_diag(sns.kdeplot, lw=3)"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "Predicting grades in maths\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCan we predict test grades in maths from demographics (ie, not from\nother grades)?\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "# A bit of feature engineering to get a numerical matrix (easily done\n# with the ColumnTransformer in scikit-learn >= 0.20)\nX = exams.drop(columns=['StudentID', 'Maths', 'Ravens', 'English'])\n# Encode gender as an integer variables\nX['Gender'] = X['Gender'] == 'Girl'\n# One-hot encode social class\nX = pd.get_dummies(X, drop_first=True)\ny = exams['Maths']\n\nfrom sklearn import ensemble\nprint(cross_val_score(ensemble.GradientBoostingRegressor(), X, y,\n                      cv=10).mean())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "We get can predict!\n\nBut there is one caveat: are we simply learning to recognive students\nacross the years? There is many implicit informations about students:\nnotably in the school ID and the class ID.\n\nTo test for this, we can make sure that we have different students in\nthe train and the test set\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "execution_count": null, 
      "cell_type": "code", 
      "source": [
        "from sklearn import model_selection\ncv = model_selection.GroupKFold(10)\n\nprint(cross_val_score(ensemble.GradientBoostingRegressor(), X, y,\n                      cv=cv, groups=exams['StudentID']).mean())"
      ], 
      "outputs": [], 
      "metadata": {
        "collapsed": false
      }
    }, 
    {
      "source": [
        "It works better!\n\nThe classifier learns better to generalize, probably by learning\nstronger invariances from the repeated measures on the students\n\n\n\n"
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2", 
      "name": "python2", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "2.7.15", 
      "pygments_lexer": "ipython2", 
      "codemirror_mode": {
        "version": 2, 
        "name": "ipython"
      }
    }
  }
}