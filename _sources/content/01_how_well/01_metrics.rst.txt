.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_content_01_how_well_01_metrics.py>` to download the full example code or run this example in your browser via Binder
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_content_01_how_well_01_metrics.py:


Metrics to judge the sucess of a model
=======================================

Pro & cons of various performance metrics.

.. contents::
    :local:

The simple way to use a scoring metric during cross-validation is
via the `scoring` parameter of
:func:`sklearn.model_selection.cross_val_score`.


Regression settings
-----------------------

The Boston housing data
........................



.. code-block:: python


    from sklearn import datasets
    boston = datasets.load_boston()

    # Shuffle the data
    from sklearn.utils import shuffle
    data, target = shuffle(boston.data, boston.target, random_state=0)







A quick plot of how each feature is related to the target



.. code-block:: python

    from matplotlib import pyplot as plt

    for feature, name in zip(data.T, boston.feature_names):
        plt.figure(figsize=(4, 3))
        plt.scatter(feature, target)
        plt.xlabel(name, size=22)
        plt.ylabel('Price (US$)', size=22)
        plt.tight_layout()




.. rst-class:: sphx-glr-horizontal


    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_001.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_002.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_003.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_004.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_005.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_006.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_007.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_008.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_009.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_010.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_011.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_012.png
            :class: sphx-glr-multi-img

    *

      .. image:: /content/01_how_well/images/sphx_glr_01_metrics_013.png
            :class: sphx-glr-multi-img




We will be using a random forest regressor to predict the price



.. code-block:: python

    from sklearn.ensemble import RandomForestRegressor
    regressor = RandomForestRegressor()







Explained variance vs Mean Square Error
.......................................

The default score is explained variance



.. code-block:: python

    from sklearn.model_selection import cross_val_score
    print(cross_val_score(regressor, data, target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.82694643  0.85253877  0.83741138]


Explained variance is convienent because it has a natural scaling: 1 is
perfect prediction, and 0 is around chance

Now let us see which houses are easier to predict:

Not along the Charles river (feature 3)



.. code-block:: python

    print(cross_val_score(regressor, data[data[:, 3] == 0],
                          target[data[:, 3] == 0]))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.83322162  0.89980524  0.82376006]


Along the Charles river



.. code-block:: python

    print(cross_val_score(regressor, data[data[:, 3] == 1],
                          target[data[:, 3] == 1]))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.57820147 -0.16637666  0.68166276]


So the houses along the Charles are harder to predict?

It's not so easy to conclude this from the explained variance: in two
different sets of observations, the variance of the target differs, and
the explained variance is a relative measure

**MSE**: We can use the mean squared error (here negated)

Not along the Charles river



.. code-block:: python

    print(cross_val_score(regressor, data[data[:, 3] == 0],
                          target[data[:, 3] == 0],
                          scoring='neg_mean_squared_error'))





.. rst-class:: sphx-glr-script-out

 Out::

    [-13.46014076  -7.2695     -13.03670764]


Along the Charles river



.. code-block:: python

    print(cross_val_score(regressor, data[data[:, 3] == 1],
                          target[data[:, 3] == 1],
                          scoring='neg_mean_squared_error'))





.. rst-class:: sphx-glr-script-out

 Out::

    [ -78.55499167 -109.24950833 -126.26929091]


So the error is larger along the Charles river


Mean Squared Error versus Mean Absolute Error
..................................................

What if we want to report an error in dollars, meaningful for an
application?

The Mean Absolute Error is useful for this goal



.. code-block:: python

    print(cross_val_score(regressor, data, target,
                          scoring='neg_mean_absolute_error'))





.. rst-class:: sphx-glr-script-out

 Out::

    [-2.40242604 -2.21615385 -2.75047619]


Summary
.........

* **explained variance**: scaled with regards to chance: 1 = perfect,
  0 = around chance, but it shouldn't used to compare predictions
  across datasets

* **mean absolute error**: enables comparison across datasets in the
  units of the target


Classification settings
-----------------------

The digits data
.................



.. code-block:: python

    digits = datasets.load_digits()
    # Let us try to detect sevens:
    sevens = (digits.target == 7)

    from sklearn.ensemble import RandomForestClassifier
    classifier = RandomForestClassifier()







Accuracy and its shortcomings
.............................

The default metric is the accuracy: the averaged fraction of success.
It takes values between 0 and 1, where 1 is perfect prediction



.. code-block:: python

    print(cross_val_score(classifier, digits.data, sevens))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.975       0.97161937  0.98996656]


However, a stupid classifier can each good prediction wit imbalanced
classes



.. code-block:: python

    from sklearn.dummy import DummyClassifier
    most_frequent = DummyClassifier(strategy='most_frequent')
    print(cross_val_score(most_frequent, digits.data, sevens))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.9         0.89983306  0.90133779]


Balanced accuracy (available in development scikit-learn versions)
fixes this, but can have surprising behaviors, such as being negative


Precision, recall, and their shortcomings
..........................................

We can measure separately false detection and misses

**Precision**: Precision counts the ratio of detections that are
correct



.. code-block:: python

    print(cross_val_score(classifier, digits.data, sevens,
                          scoring='precision'))




.. rst-class:: sphx-glr-script-out

 Out::

    [ 1.  1.  1.]


Our classifier has a good precision: most of the sevens that it
predicts are really sevens.

As predicting the most frequent never predicts sevens, precision is ill
defined. Scikit-learn puts it to zero



.. code-block:: python

    print(cross_val_score(most_frequent, digits.data, sevens,
                          scoring='precision'))






.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.  0.  0.]


**Recall**: Recall counts the fraction of class 1 actually detected



.. code-block:: python

    print(cross_val_score(classifier, digits.data, sevens, scoring='recall'))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.68333333  0.85        0.89830508]


Our recall isn't as good: we miss many sevens

But predicting the most frequent never predicts sevens:



.. code-block:: python

    print(cross_val_score(most_frequent, digits.data, sevens, scoring='recall'))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.  0.  0.]


**Note**: Measuring only the precision without the recall makes no
sense, it is easy to maximize one at the cost of the other. Ideally,
classifiers should be compared on a precision at a given recall


Area under the ROC curve
..........................

If the classifier provides a decision function that can be thresholded
to control false positives versus false negatives, the ROC curve
summarise the different tradeoffs that can be achieved by varying this
threshold.

Its Area Under the Curve (AUC) is a useful metric where 1 is perfect
prediction and .5 is chance, independently of class imbalance



.. code-block:: python

    print(cross_val_score(classifier, digits.data, sevens, scoring='roc_auc'))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.99441358  0.99868584  0.99885224]



.. code-block:: python

    print(cross_val_score(most_frequent, digits.data, sevens, scoring='roc_auc'))






.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.5  0.5  0.5]


Average precision
..................

When the classifier exposes its unthresholded decision, another
interesting metric is the average precision for all recall. Compared to
ROC AUC it has a more linear behavior for very rare classes. Indeed,
with very rare classes, small changes in the ROC AUC may mean large
changes in terms of precision



.. code-block:: python

    print(cross_val_score(classifier, digits.data, sevens,
                          scoring='average_precision'))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.9428459   0.97449684  0.97616341]


Naive decisions are no longer at .5



.. code-block:: python

    print(cross_val_score(most_frequent, digits.data, sevens,
                          scoring='average_precision'))






.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.1         0.10016694  0.09866221]


Multiclass and multilabel settings
...................................

To simplify the discussion, we have reduced the problem to detecting
sevens, but maybe it is more interesting to predict the digit: a
10-class classification problem

**Accuracy** The accuracy is naturally defined in such multiclass settings



.. code-block:: python

    print(cross_val_score(classifier, digits.data, digits.target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.89368771  0.91986644  0.90436242]


The most frequent label is no longer a very interesting baseline



.. code-block:: python

    random_choice = DummyClassifier()
    print(cross_val_score(random_choice, digits.data, digits.target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.10299003  0.0884808   0.08221477]


Precision and recall need the notion of specific class to detect
(called positive class) and are not that easily defined in these
settings, hence ROC AUC cannot be easily computed.

These notions are however well defined in a multi-label problem.
In such a problem, the goal is to assign one or more labels to each
instance, as opposed to a multiclass. A multiclass problem can be
turned into a multilabel one, though the prediction will then be
slightly different



.. code-block:: python

    from sklearn.preprocessing import LabelBinarizer
    digit_labels = LabelBinarizer().fit_transform(digits.target)
    print(digit_labels[:15])





.. rst-class:: sphx-glr-script-out

 Out::

    [[1 0 0 0 0 0 0 0 0 0]
     [0 1 0 0 0 0 0 0 0 0]
     [0 0 1 0 0 0 0 0 0 0]
     [0 0 0 1 0 0 0 0 0 0]
     [0 0 0 0 1 0 0 0 0 0]
     [0 0 0 0 0 1 0 0 0 0]
     [0 0 0 0 0 0 1 0 0 0]
     [0 0 0 0 0 0 0 1 0 0]
     [0 0 0 0 0 0 0 0 1 0]
     [0 0 0 0 0 0 0 0 0 1]
     [1 0 0 0 0 0 0 0 0 0]
     [0 1 0 0 0 0 0 0 0 0]
     [0 0 1 0 0 0 0 0 0 0]
     [0 0 0 1 0 0 0 0 0 0]
     [0 0 0 0 1 0 0 0 0 0]]


The ROC AUC can then be computed for each label



.. code-block:: python

    print(cross_val_score(classifier, digits.data, digit_labels,
                          scoring='roc_auc'))




.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.9874304   0.9870478   0.98160379]


as well as the average precision



.. code-block:: python

    print(cross_val_score(classifier, digits.data, digit_labels,
                          scoring='average_precision'))




.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.94547594  0.94510265  0.93917088]


Note that the confusion between classes may not well be captured in
Such a measure, as in multiclass predictions are exclusive, and not
in multilabel.


Summary
..........

Class imbalance and the tradeoffs between accepting many misses or many
false detections are the things to keep in mind in classification.

In single-class settings, ROC AUC and average precision give nice
summaries to compare classifiers when the threshold can be varied. In
multiclass settings, this is harder, unless we are willing to consider
the problem as multiple single-class problems (one-vs-all).

____________________


**Total running time of the script:** ( 0 minutes  2.600 seconds)


.. _sphx_glr_download_content_01_how_well_01_metrics.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: https://static.mybinder.org/badge.svg
      :target: https://mybinder.org/v2/gh/gaelvaroquaux/interpreting_ml_tuto/master?filepath=_downloads/01_metrics.ipynb
      :width: 150 px


  .. container:: sphx-glr-download

     :download:`Download Python source code: 01_metrics.py <01_metrics.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: 01_metrics.ipynb <01_metrics.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
