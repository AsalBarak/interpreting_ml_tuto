.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_content_01_how_well_02_cross_validation.py>` to download the full example code or run this example in your browser via Binder
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_content_01_how_well_02_cross_validation.py:


Cross-validation: some gotchas
===============================

Cross-validation is the ubiquitous test of a machine learning model. Yet
many things can go wrong.



Uncertainty of measured accuracy
--------------------------------

Simple experiments reveal variations in cross_val_score
........................................................

The first thing to have in mind is that the results of a
cross-validation are noisy estimate of the real prediction accuracy

Let us create a simple artificial data



.. code-block:: python

    from sklearn import datasets, discriminant_analysis
    import numpy as np
    np.random.seed(0)
    data, target = datasets.make_blobs(centers=[(0, 0), (0, 1)], n_samples=100)
    classifier = discriminant_analysis.LinearDiscriminantAnalysis()







One cross-validation gives spread out measures



.. code-block:: python

    from sklearn.model_selection import cross_val_score
    print(cross_val_score(classifier, data, target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.64705882  0.67647059  0.84375   ]


What if we try different random shuffles of the data?



.. code-block:: python

    from sklearn import utils
    for _ in range(10):
        data, target = utils.shuffle(data, target)
        print(cross_val_score(classifier, data, target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.76470588  0.70588235  0.65625   ]
    [ 0.70588235  0.67647059  0.75      ]
    [ 0.73529412  0.64705882  0.71875   ]
    [ 0.70588235  0.58823529  0.8125    ]
    [ 0.67647059  0.73529412  0.71875   ]
    [ 0.70588235  0.64705882  0.75      ]
    [ 0.67647059  0.67647059  0.71875   ]
    [ 0.70588235  0.61764706  0.8125    ]
    [ 0.76470588  0.76470588  0.59375   ]
    [ 0.76470588  0.61764706  0.625     ]


A simple probabilistic model
.............................

A sample probabilistic model gives the distribution of observed error:
if the classification rate is p, the observed distribution of correct
classifications on a set of size follows a binomial distribution



.. code-block:: python

    from scipy import stats
    n = len(data)
    distrib = stats.binom(n=n, p=.7)







We can plot it:



.. code-block:: python

    from matplotlib import pyplot as plt
    plt.figure(figsize=(6, 3))
    plt.plot(np.linspace(0, 1, n), distrib.pmf(np.arange(0, n)))




.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_001.png
    :class: sphx-glr-single-img




It is wide, because there are not that many samples to mesure the error
upon: this is a small dataset.

We can look at the interval in which 95% of the observed accuracy lies
for different sample sizes



.. code-block:: python

    for n in [100, 1000, 10000, 100000, 1000000]:
        distrib = stats.binom(n, .7)
        interval = (distrib.isf(.025) - distrib.isf(.975)) / n
        print("Size: {0: 8}  | interval: {1}%".format(n, 100 * interval))





.. rst-class:: sphx-glr-script-out

 Out::

    Size:      100  | interval: 18.0%
    Size:     1000  | interval: 5.7%
    Size:    10000  | interval: 1.8%
    Size:   100000  | interval: 0.568%
    Size:  1000000  | interval: 0.1796%


At 100 000 samples, 5% of the observed classification accuracy still
fall more than .5% away of the true rate.

**Keep in mind that cross-val is a noisy measure**


Empirical distribution of cross-validation scores
.....................................................

We can sample the distribution of scores using cross-validation
iterators based on subsampling, such as
:class:`sklearn.model_selection.ShuffleSplit`, with many many splits



.. code-block:: python

    from sklearn import model_selection
    cv = model_selection.ShuffleSplit(n_splits=200)
    scores = cross_val_score(classifier, data, target, cv=cv)

    import seaborn as sns
    plt.figure(figsize=(6, 3))
    sns.distplot(scores)
    plt.xlim(0, 1)




.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_002.png
    :class: sphx-glr-single-img




The empirical distribution is broader than the theoretical one. This
can be explained by the fact that as we are retraining the model on
each fold, it actually fluctuates due the sampling noise in the
training data, while the model above only accounts for sampling noise
in the test data.

The situation does get better with more data:



.. code-block:: python

    data, target = datasets.make_blobs(centers=[(0, 0), (0, 1)], n_samples=1000)

    scores = cross_val_score(classifier, data, target, cv=cv)

    plt.figure(figsize=(6, 3))
    sns.distplot(scores)
    plt.xlim(0, 1)
    plt.title("Distribution with 1000 data points")




.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_003.png
    :class: sphx-glr-single-img




The distribution is still very broader

**Testing the observed scores**

Importantly, the standard error of the mean across folds is not a good
measure of this error, as the different data folds are not independent.
For instance, doing many random splits will can reduce the variance
arbitrarily, but does not provide actually new data points



.. code-block:: python

    from scipy import stats

    plt.figure(figsize=(6, 3))
    sns.distplot(scores)
    plt.axvline(np.mean(scores), color='k')
    plt.axvline(np.mean(scores) + np.std(scores), color='b', label='std')
    plt.axvline(np.mean(scores) - np.std(scores), color='b')
    plt.axvline(np.mean(scores) + stats.sem(scores), color='r', label='SEM')
    plt.axvline(np.mean(scores) - stats.sem(scores), color='r')
    plt.legend(loc='best')
    plt.xlim(0, 1)
    plt.title("Distribution with 1000 data points")




.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_004.png
    :class: sphx-glr-single-img




Measuring baselines and chance
-------------------------------

Because of class imbalances, or confounding effects, it is easy to get
it wrong it terms of what constitutes chances. There are two approaches
to measure peformances of baselines or chance:

Let's go back to simple generated data:



.. code-block:: python

    data, target = datasets.make_blobs(centers=[(0, 0), (0, 1)])







**DummyClassifier** The dummy classifier:
:class:`sklearn.dummy.DummyClassifier`, with different strategies to
provide simple baselines



.. code-block:: python

    from sklearn.dummy import DummyClassifier
    dummy = DummyClassifier(strategy="stratified")
    print(cross_val_score(dummy, data, target))





.. rst-class:: sphx-glr-script-out

 Out::

    [ 0.47058824  0.5         0.40625   ]


**Chance level** To measure actual chance, the most robust approach is
to use permutations:
:func:`sklearn.model_selection.permutation_test_score`, which is used
as cross_val_score



.. code-block:: python

    from sklearn.model_selection import permutation_test_score
    score, permuted_scores, p_value = permutation_test_score(classifier, data, target)
    print("Classifier score: {0},\np value: {1}\nPermutation scores {2}"
            .format(score, p_value, permuted_scores))






.. rst-class:: sphx-glr-script-out

 Out::

    Classifier score: 0.660539215686,
    p value: 0.019801980198
    Permutation scores [ 0.57965686  0.58088235  0.58884804  0.49203431  0.51041667  0.59129902
      0.45036765  0.44914216  0.46078431  0.59129902  0.37928922  0.53982843
      0.48039216  0.5189951   0.5189951   0.53982843  0.62009804  0.56066176
      0.50122549  0.50796569  0.58026961  0.50980392  0.57781863  0.52941176
      0.51838235  0.59007353  0.49019608  0.44852941  0.36887255  0.53125
      0.48958333  0.46078431  0.53553922  0.55698529  0.43014706  0.60968137
      0.61090686  0.40073529  0.46752451  0.45833333  0.41973039  0.55821078
      0.48958333  0.50796569  0.68198529  0.57904412  0.4877451   0.44791667
      0.40992647  0.46813725  0.49080882  0.47916667  0.56985294  0.48958333
      0.56066176  0.52941176  0.55147059  0.53125     0.52144608  0.59068627
      0.43933824  0.41973039  0.53063725  0.35110294  0.63112745  0.57107843
      0.4620098   0.54963235  0.53002451  0.47794118  0.48223039  0.49938725
      0.53982843  0.54227941  0.47120098  0.46936275  0.43995098  0.48161765
      0.56985294  0.37867647  0.46017157  0.55943627  0.53982843  0.60906863
      0.53860294  0.48161765  0.43014706  0.55882353  0.44914216  0.58210784
      0.42095588  0.57965686  0.56066176  0.46139706  0.61090686  0.5502451
      0.47977941  0.49080882  0.41973039  0.52941176]


Cross-validation with non iid data
-----------------------------------

Stock market: time series
...........................

Download and load the data



.. code-block:: python

    import pandas as pd
    import os
    # Python 2 vs Python 3:
    try:
        from urllib.request import urlretrieve
    except ImportError:
        from urllib import urlretrieve

    symbols = {'TOT': 'Total', 'XOM': 'Exxon', 'CVX': 'Chevron',
               'COP': 'ConocoPhillips', 'VLO': 'Valero Energy'}

    quotes = pd.DataFrame()

    for symbol, name in symbols.items():
        url = ('https://raw.githubusercontent.com/scikit-learn/examples-data/'
               'master/financial-data/{}.csv')
        filename = "{}.csv".format(symbol)
        if not os.path.exists(filename):
            urlretrieve(url.format(symbol), filename)
        this_quote = pd.read_csv(filename)
        quotes[name] = this_quote['open']







Predict 'Chevron' from the others



.. code-block:: python

    from sklearn import linear_model, model_selection, ensemble
    cv = model_selection.ShuffleSplit(random_state=0)
    print(cross_val_score(linear_model.RidgeCV(),
                          quotes.drop(columns=['Chevron']),
                          quotes['Chevron'],
                          cv=cv).mean())





.. rst-class:: sphx-glr-script-out

 Out::

    0.255791000942


Is this a robust prediction?

Does it cary over across quarters?



.. code-block:: python

    quarters = pd.to_datetime(this_quote['date']).dt.to_period('Q')
    cv = model_selection.LeaveOneGroupOut()

    print(cross_val_score(linear_model.RidgeCV(),
                          quotes.drop(columns=['Chevron']),
                          quotes['Chevron'],
                          cv=cv, groups=quarters).mean())





.. rst-class:: sphx-glr-script-out

 Out::

    -55.0821056887


The problem that we are facing here is the auto-correlation in the
data: these datasets are **time-series**.



.. code-block:: python

    quotes_with_dates = pd.concat((quotes, this_quote['date']),
                                  axis=1).set_index('date')
    quotes_with_dates.plot()




.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_005.png
    :class: sphx-glr-single-img




If the goal is to do forecasting, than prediction should be done in the
future, for instance using
:class:`sklearn.model_selection.TimeSeriesSplit`

Can we do forecasting: predict the future?



.. code-block:: python


    cv = model_selection.TimeSeriesSplit(n_splits=quarters.nunique())

    print(cross_val_score(linear_model.RidgeCV(),
                          quotes.drop(columns=['Chevron']),
                          quotes['Chevron'],
                          cv=cv, groups=quarters).mean())





.. rst-class:: sphx-glr-script-out

 Out::

    -177.720872861


No. This prediction is abysmal


School grades: repeated measures
.................................

Let us look at another dependency structure across samples: repeated
measures. This is often often in longitudinal data. Here we are looking
at grades of school students, across the years.

Download and load the data
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Download some data on grades across several schools (centers)

The junior school data, originally from http://www.bristol.ac.uk/cmm/learning/support/datasets/



.. code-block:: python

    if not os.path.exists('exams.csv.gz'):
        # Download the file if it is not present
        urlretrieve('https://raw.githubusercontent.com/GaelVaroquaux/interpreting_ml_tuto/blob/master/src/01_how_well/exams.csv.gz',
                    filename)
    exams = pd.read_csv('exams.csv.gz')

    # Select data for students present all three years
    continuing_students = exams.StudentID.value_counts()
    continuing_students = continuing_students[continuing_students > 2].index
    exams = exams[exams.StudentID.isin(continuing_students)]







Visualizing factor of grades
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Grade at tests in in exams depend on socio-economic status, year at
school, ...



.. code-block:: python

    import seaborn as sns
    g = sns.PairGrid(exams.drop(columns=['StudentID']),
                     diag_sharey=False)
    g.map_lower(sns.kdeplot)
    g.map_upper(plt.scatter, s=2)
    g.map_diag(sns.kdeplot, lw=3)





.. image:: /content/01_how_well/images/sphx_glr_02_cross_validation_006.png
    :class: sphx-glr-single-img




Predicting grades in maths
~~~~~~~~~~~~~~~~~~~~~~~~~~~

Can we predict test grades in maths from demographics (ie, not from
other grades)?



.. code-block:: python


    # A bit of feature engineering to get a numerical matrix (easily done
    # with the ColumnTransformer in scikit-learn >= 0.20)
    X = exams.drop(columns=['StudentID', 'Maths', 'Ravens', 'English'])
    # Encode gender as an integer variables
    X['Gender'] = X['Gender'] == 'Girl'
    # One-hot encode social class
    X = pd.get_dummies(X, drop_first=True)
    y = exams['Maths']

    from sklearn import ensemble
    print(cross_val_score(ensemble.GradientBoostingRegressor(), X, y,
                          cv=10).mean())





.. rst-class:: sphx-glr-script-out

 Out::

    0.0725511722169


We can predict!

But there is one caveat: are we simply learning to recognive students
across the years? There is many implicit informations about students:
notably in the school ID and the class ID.

To test for this, we can make sure that we have different students in
the train and the test set



.. code-block:: python

    from sklearn import model_selection
    cv = model_selection.GroupKFold(10)

    print(cross_val_score(ensemble.GradientBoostingRegressor(), X, y,
                          cv=cv, groups=exams['StudentID']).mean())





.. rst-class:: sphx-glr-script-out

 Out::

    0.14218996362


It works better!

The classifier learns better to generalize, probably by learning
stronger invariances from the repeated measures on the students

Summary
.......

Samples often have a dependency structure, such a with time-series, or
with repeated measures. To have a meaningful measure of prediction
error, the link between the train and the test set must match the
important one for the application. In time-series prediction, it must
be in the future. To learn a predictor of the success of an individual
from demographics, it might be more relevant to predict across
individuals. If the variance across individuals is much larger than the
variance across repeated measurement, as in many biomedical
applications, the choice of cross-validation strategy may make a huge
difference.


**Total running time of the script:** ( 0 minutes  27.177 seconds)


.. _sphx_glr_download_content_01_how_well_02_cross_validation.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: https://static.mybinder.org/badge.svg
      :target: https://mybinder.org/v2/gh/gaelvaroquaux/interpreting_ml_tuto/master?filepath=_downloads/02_cross_validation.ipynb
      :width: 150 px


  .. container:: sphx-glr-download

     :download:`Download Python source code: 02_cross_validation.py <02_cross_validation.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: 02_cross_validation.ipynb <02_cross_validation.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
